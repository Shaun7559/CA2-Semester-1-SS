{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc469a97",
   "metadata": {},
   "source": [
    "# Creation of the Web scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c6d95",
   "metadata": {},
   "source": [
    "## Import the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fab6759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T16:14:42.576996Z",
     "start_time": "2022-05-18T16:14:42.165493Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b5033",
   "metadata": {},
   "source": [
    "## Create the driver (Chrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36820d19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T16:14:44.630933Z",
     "start_time": "2022-05-18T16:14:43.509602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaun\\AppData\\Local\\Temp/ipykernel_14640/2026946682.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r\"C:\\Users\\shaun\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# Creating the web driver that will scrape the websites\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\shaun\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b67b3",
   "metadata": {},
   "source": [
    "# Producer Sentiment - IFA Web Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2cc7d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-18T16:14:57.764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaun\\AppData\\Local\\Temp/ipykernel_14640/2289504164.py:16: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  elems = driver.find_elements_by_css_selector(\".col-sm-12 [href]\")\n",
      "C:\\Users\\shaun\\AppData\\Local\\Temp/ipykernel_14640/2289504164.py:21: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  page = driver.find_element_by_xpath(\"/html/body\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The crawler was unable to collect information on https://www.ifa.ie/beef-farmers-online-meeting/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/the-eu-beef-crisis-explained/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/brexit-beef-campaign/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/beef-talks-agreement/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/beef-data-genomics-programme-bdgp/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/beef-finisher-scheme-how-to-apply/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/beef-finisher-payment-scheme/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/ifa-dairy-calf-webinar/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/save-irish-farming/\n",
      "The crawler was unable to collect information on https://www.ifa.ie/cap2023/\n"
     ]
    }
   ],
   "source": [
    "# This scraper uses Selenium with a chrome driver to search the IFA website\n",
    "# HTML element class names, it then loops through a set maximum number of pages\n",
    "# amd returns any result contained within the keywords list\n",
    "\n",
    "TargetUrl = 'https://www.ifa.ie/page/'\n",
    "keywords = [\"beef\", \"cattle\", \"mutton\", \"sheep\", \"prices\", \"price\", \"poultry\",\n",
    "            \"CAP\", \"common agriculutral policy\", \"pig\", \"pork\", \"meat\"]\n",
    "\n",
    "maxobservations = 100\n",
    "sentimentdata = pd.DataFrame(columns=['Link', 'Date', 'Title', 'Text'])\n",
    "\n",
    "for word in keywords:\n",
    "    for i in range(maxobservations):\n",
    "        SearchUrl = TargetUrl + str(i+1) + '/?s=' + word\n",
    "        driver.get(SearchUrl)\n",
    "        elems = driver.find_elements_by_css_selector(\".col-sm-12 [href]\")\n",
    "        links = [elem.get_attribute('href') for elem in elems]\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            try:\n",
    "                page = driver.find_element_by_xpath(\"/html/body\")\n",
    "                sentimentdata = sentimentdata.append({'Link': link,\n",
    "                                           'Date': page.find_element(By.CLASS_NAME, 'entry-date').text,\n",
    "                                           'Title': page.find_element(By.CLASS_NAME, 'entry-title').text,\n",
    "                                           'Text': page.find_element(By.CLASS_NAME, 'single-content').text},\n",
    "                                          ignore_index=True)\n",
    "            except:\n",
    "                print('The crawler was unable to collect information on {}'.format(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5b54c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-18T16:14:58.540Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "sentimentdata.to_csv(\"C:\\\\Users\\\\shaun\\\\Desktop\\\\IFA_sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ad089",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-18T16:14:59.309Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "sentimentdata.to_csv(\"C:\\\\Users\\\\shaun\\\\OneDrive\\\\MSc Data Analytics\\\\CA2 Semester 1\\\\Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d45693",
   "metadata": {},
   "source": [
    "# Consumer Sentiment - Examiner Web Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500666f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T15:58:50.336949Z",
     "start_time": "2022-05-18T15:58:17.871089Z"
    }
   },
   "outputs": [],
   "source": [
    "# This scraper uses Selenium with a chrome driver to search the IFA website\n",
    "# HTML element class names, it then loops through a set maximum number of pages\n",
    "# amd returns any result contained within the keywords list\n",
    "\n",
    "TargetUrl = 'https://www.ifa.ie/page/'\n",
    "keywords = [\"beef\", \"cattle\", \"mutton\", \"sheep\", \"prices\", \"price\", \"poultry\",\n",
    "            \"CAP\", \"common agriculutral policy\", \"pig\", \"pork\", \"meat\"]\n",
    "\n",
    "maxobservations = 1\n",
    "sentimentdata = pd.DataFrame(columns=['Link', 'Date', 'Title', 'Text'])\n",
    "\n",
    "for word in keywords:\n",
    "    for i in range(maxobservations):\n",
    "        SearchUrl = TargetUrl + str(i+1) + '/?s=' + word\n",
    "        driver.get(SearchUrl)\n",
    "        elems = driver.find_elements_by_css_selector(\".col-sm-12 [href]\")\n",
    "        links = [elem.get_attribute('href') for elem in elems]\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            try:\n",
    "                page = driver.find_element_by_xpath(\"/html/body\")\n",
    "                sentimentdata = sentimentdata.append({'Link': link,\n",
    "                                           'Date': page.find_element(By.CLASS_NAME, 'entry-date').text,\n",
    "                                           'Title': page.find_element(By.CLASS_NAME, 'entry-title').text,\n",
    "                                           'Text': page.find_element(By.CLASS_NAME, 'single-content').text},\n",
    "                                          ignore_index=True)\n",
    "            except:\n",
    "                print('The crawler was unable to collect information on {}'.format(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f05f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "124f121c",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf951e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T16:09:31.887444Z",
     "start_time": "2022-05-18T16:09:29.792210Z"
    }
   },
   "outputs": [],
   "source": [
    "# This scraper uses Selenium with a chrome driver to search the IFA website\n",
    "# HTML element class names, it then loops through a set maximum number of pages\n",
    "# amd returns any result contained within the keywords list\n",
    "\n",
    "TargetUrl = 'https://www.irishexaminer.com/page/'\n",
    "keywords = [\"beef\", \"cattle\", \"mutton\", \"sheep\", \"prices\", \"price\", \"poultry\",\n",
    "            \"CAP\", \"common agriculutral policy\", \"pig\", \"pork\", \"meat\"]\n",
    "\n",
    "maxobservations = 1\n",
    "consumerdata = pd.DataFrame(columns=['Link', 'Date', 'Title', 'Text'])\n",
    "\n",
    "for word in keywords:\n",
    "    for i in range(maxobservations):\n",
    "        SearchUrl = TargetUrl + str(i+1) + '/?s=' + word\n",
    "        driver.get(SearchUrl)\n",
    "        elems = driver.find_elements_by_css_selector(\".col-sm-12 [href]\")\n",
    "        links = [elem.get_attribute('href') for elem in elems]\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            try:\n",
    "                page = driver.find_element_by_xpath(\"/html/body\")\n",
    "                consumerdata = consumerdata.append({'Link': link,\n",
    "                                           'Date': page.find_element(By.CLASS_NAME, 'entry-date').text,\n",
    "                                           'Title': page.find_element(By.CLASS_NAME, 'entry-title').text,\n",
    "                                           'Text': page.find_element(By.CLASS_NAME, 'single-content').text},\n",
    "                                          ignore_index=True)\n",
    "            except:\n",
    "                print('The crawler was unable to collect information on {}'.format(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c40b64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T14:58:34.868920Z",
     "start_time": "2022-05-18T14:58:34.848920Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "sentimentdata.to_csv(\"C:\\\\Users\\\\shaun\\\\Desktop\\\\IFA_sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc830a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e02bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622b132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29a6f7f",
   "metadata": {},
   "source": [
    "## Creating the web scrape as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae5adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T15:56:22.739880Z",
     "start_time": "2022-05-18T15:56:22.734880Z"
    }
   },
   "outputs": [],
   "source": [
    "def selenium_harvest(TargetUrl, maxobservations, Date_class, Title_class,\n",
    "                    Text_class):\n",
    "    TargetUrl = TargetUrl\n",
    "    keywords = [\"beef\", \"cattle\", \"mutton\", \"sheep\", \"prices\", \"price\", \"poultry\",\n",
    "                \"CAP\", \"common agriculutral policy\", \"pig\", \"pork\", \"meat\"]\n",
    "\n",
    "    maxobservations = maxobservations\n",
    "    dataframe = pd.DataFrame(columns=['Link', 'Date', 'Title', 'Text'])\n",
    "\n",
    "    for word in keywords:\n",
    "        for i in range(maxobservations):\n",
    "            SearchUrl = TargetUrl + str(i+1) + '/?s=' + word\n",
    "            driver.get(SearchUrl)\n",
    "            elems = driver.find_elements_by_css_selector(\".col-sm-12 [href]\")\n",
    "            links = [elem.get_attribute('href') for elem in elems]\n",
    "            for link in links:\n",
    "                driver.get(link)\n",
    "                try:\n",
    "                    page = driver.find_element_by_xpath(\"/html/body\")\n",
    "                    dataframe = dataframe.append({'Link': link,\n",
    "                                               'Date': page.find_element(By.CLASS_NAME, Date_class).text,\n",
    "                                               'Title': page.find_element(By.CLASS_NAME, Title_class).text,\n",
    "                                               'Text': page.find_element(By.CLASS_NAME, Text_class).text},\n",
    "                                              ignore_index=True)\n",
    "                except:\n",
    "                    print('The crawler was unable to collect information on {}'.format(link))\n",
    "        return dataframe\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db57c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T15:55:23.307882Z",
     "start_time": "2022-05-18T15:55:23.302868Z"
    }
   },
   "outputs": [],
   "source": [
    "def selenium_harvest(TargetUrl, maxobservations, Date_class, Title_class,\n",
    "                    Text_class):\n",
    "    TargetUrl = TargetUrl\n",
    "    keywords = [\"beef\"]\n",
    "\n",
    "    maxobservations = maxobservations\n",
    "    dataframe = pd.DataFrame(columns=['Link', 'Date', 'Title', 'Text'])\n",
    "\n",
    "    for word in keywords:\n",
    "        for i in range(maxobservations):\n",
    "            SearchUrl = TargetUrl + str(i+1) + '/?s=' + word\n",
    "            driver.get(SearchUrl)\n",
    "            elems = driver.find_elements_by_css_selector(\".col-sm-12 [href]\")\n",
    "            links = [elem.get_attribute('href') for elem in elems]\n",
    "            for link in links:\n",
    "                driver.get(link)\n",
    "                try:\n",
    "                    page = driver.find_element_by_xpath(\"/html/body\")\n",
    "                    dataframe = dataframe.append({'Link': link,\n",
    "                                               'Date': page.find_element(By.CLASS_NAME, Date_class).text,\n",
    "                                               'Title': page.find_element(By.CLASS_NAME, Title_class).text,\n",
    "                                               'Text': page.find_element(By.CLASS_NAME, Text_class).text},\n",
    "                                              ignore_index=True)\n",
    "                except:\n",
    "                    print('The crawler was unable to collect information on {}'.format(link))\n",
    "        return dataframe\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9190f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T15:56:33.532032Z",
     "start_time": "2022-05-18T15:56:29.503174Z"
    }
   },
   "source": [
    "test = selenium_harvest('https://www.ifa.ie/page/', 1, 'entry-date', 'entry-title',\n",
    "                    'single-content')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
